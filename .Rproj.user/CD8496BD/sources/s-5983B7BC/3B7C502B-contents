---
title: "Assignment 1"
author: "Vanessa Avila"
date: "4/14/2021"
output:
  html_document:
    theme: united
    df_print: paged
    highlight: textmate
    code_folding: show
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: inline
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1: Data Wrangling
My first step was to load the required libraries, which included tidyverse, janitor, and skimr into R.

```{r}
library(tidyverse)
library(janitor)
library(skimr)
```

### A.1 Imported Dataset into R
Next, I imported the sandyrelated dataset which was saved in a data file I created in my directory.
```{r}

df_sandy <- read_csv("data/sandyrelated.csv")
```


### A.2 Inspected the Data 
Before trimming white spaces and transforming addresses to title case, I wanted to review all of the data to identify the address related columns.

```{r}

df_sandy %>% glimpse()
```

### B. Trimmed White Spaces & Transformed Title Case 
I trimmed the white spaces and transformed the address related columns to title case.

```{r}
df_sandy_clean <- df_sandy %>% 
  janitor::clean_names()

df_sandy_clean <- df_sandy_clean %>% 
  mutate(
    incident_address = str_trim(incident_address),
    incident_address = str_to_title(incident_address),
    street_name = str_trim(street_name),
    street_name = str_to_title(street_name),
    cross_street_1 = str_trim(cross_street_1),
    cross_street_1 = str_to_title(cross_street_1),
    cross_street_2 = str_trim(cross_street_2),
    cross_street_2 = str_to_title(cross_street_2),
    intersection_street_1 = str_trim(intersection_street_1),
    intersection_street_1 = str_to_title(intersection_street_1),
    intersection_street_2 = str_trim(intersection_street_2),
    intersection_street_2 = str_to_title(intersection_street_2),
    address_type = str_trim(address_type),
    address_type = str_to_title(address_type),
    city = str_trim(city),
    city = str_to_title(city),
    community_board = str_trim(community_board),
    community_board = str_to_title(community_board),
    borough = str_trim(borough),
    borough = str_to_title(borough),
    park_borough = str_trim(park_borough),
    park_borough = str_to_title(park_borough),
    school_address = str_trim(school_address),
    school_address = str_to_title(school_address),
    school_city = str_trim(school_city),
    school_city = str_to_title(school_city),
    school_state = str_trim(school_state),
    school_state = str_to_title(school_state),
   )
```

### C.1 Convert "N/A" values to NA
I first filtered all the columns that had "N/A" or "Unspecified" entries.

```{r}
df_sandy_clean %>% 
  filter_all(any_vars(. == "N/A" | . == "Unspecified"))
```

I then converted the "N/A" and "Unspecified" variables under 10 columns to NA.

```{r}
df_sandy_na_fixed <- df_sandy_clean %>% 
  mutate(facility_type = ifelse(
    facility_type == "N/A" | facility_type == "Unspecified", NA, facility_type
  ),
    park_facility_name = ifelse(
    park_facility_name == "N/A" | park_facility_name == "Unspecified", NA, park_facility_name
  ),
  school_name = ifelse(
    school_name == "N/A" | school_name == "Unspecified", NA, school_name
  ),
  school_number = ifelse(
    school_number == "N/A" | school_number == "Unspecified", NA, school_number
  ),
  school_region = ifelse(
    school_region == "N/A" | school_region == "Unspecified", NA, school_region
  ),
  school_code = ifelse(
    school_code == "N/A" | school_code == "Unspecified", NA, school_code
  ),
  school_phone_number = ifelse(
    school_phone_number == "N/A" | school_phone_number == "Unspecified", NA, school_phone_number
  ),
  school_address = ifelse(
    school_address == "N/A" | school_address == "Unspecified", NA, school_address
  ),
  school_city = ifelse(
    school_city == "N/A" | school_city == "Unspecified", NA, school_city
  ),
  school_state = ifelse(
    school_state == "N/A" | school_state == "Unspecified", NA, school_state
  ),
  school_zip = ifelse(
    school_zip == "N/A" | school_zip == "Unspecified", NA, school_zip
  ),
  school_not_found = ifelse(
    school_not_found == "N/A" | school_not_found == "Unspecified" | school_not_found == "N", NA, school_not_found))
```

### C.2 Removed columns where the majority of cells were NA
First, I viewed a pivot of the columns missing >50% of the data and I created a value entitled columns_missing_most_data, which contained the columns that needed to be removed. 

```{r}
columns_missing_most_data <- df_sandy_na_fixed %>%
  summarise(across(everything(), ~ skimr::n_missing(.x))) %>%
  pivot_longer(cols = everything(),
               names_to = "variable",
               values_to = "missing_count") %>% 
  mutate(proportion_missing = round(missing_count / nrow(df_sandy_clean), 2)) %>% 
  arrange(desc(missing_count)) %>% 
  # Detect columns that were missing more than 50% of the data
  filter(proportion_missing > .5) %>% 
  pull(variable)

print(message("Columns missing 50%+ data:"))
columns_missing_most_data
```

Next, I removed the columns with >50% of missing data and renamed the dataframe "df_sandy_nacolumns_removed".

```{r}
df_sandy_nacolumns_removed <- df_sandy_na_fixed %>% 
  select(-columns_missing_most_data)
```

### D. Title case

I created a new dataframe object named "df_sandy_final" and converted the City column to title case.

```{r}
df_sandy_final <- df_sandy_nacolumns_removed %>% 
  mutate(city = str_to_title(city))
```

### E. Looked for at least two other data cleaning opportunities and executed using R

**Removed Duplicate Columns** from the dataframe once I identified that "borough" and "park_borough" were essentially duplicates. In an effort to further refine the dataset, I removed "park_borough" through the use of a subset command.

```{r}
df_sandy_final <- subset (df_sandy_final, select = -park_borough)
```

**Identified & Removed Erroneous Dates:**

In looking closely at the values in the dataset, I noticed that under the "closed_date" 110 entries were listed as "01/01/1900 12:00:00 AM". I viewed them against their corresponding "created_date", and for all 110 entries the created date was in 2012. Therefore, the close date is erroneously listed, since it cannot be closed before it was created. In an effort to further clean the dataset, I changed those values to NA.

```{r}
  df_sandy_final1 <- df_sandy_final %>% 
  mutate(closed_date = ifelse(
    closed_date == "01/01/1900 12:00:00 AM", NA, closed_date)
  )
```

## Part 2: Relational Model

### A. Downloaded Sakila dataset
I downloaded the Sakila dataset into my computer and unzipped the files.

### B. Opened MySQL workbench and executed the scripts
I opened MySQL and executed the scripts - sakila-schema.sql followed by sakila-data.sql. The following is a screenshot of the executed scripts. You will see a tab for both "sakila-schema" and "sakila-data".
![](sakila.png)

### C. Reverse Engineered the Database and Generated the EER model
I reverse engineered the database to generate the EER Model for the sakila database. The following screenshot illustrates the EER model.
![](EER.png)

### D. Modified the EER model to add a new lookup table : payment_type

Within the EER model, I was able to add a new lookup table entitled "payment_type". I input three attributes into the table, which included payment_type_id, method, and description. I then added payment_type_id as a foreign key in the Payment table. As illustrated in the screenshot below, the new payment_type table has a 1 to Many relationship with the payment table.

![](eer_model.png)

### E. Payment Table
Screenshot of the payment table with field attributes
![](paymenttable.png)

## Part 3: Relational Algebra
Utilizing the Sakila dataset, I completed the following relational algebra syntax questions:

![](relationalalgebra.png)

## Part 4: Normalization

### A.For the table below, provide examples of insertion, deletion, and modification anomalies.

**Insertion anomaly:** the unnormalized table contains non-atomic values, missing primary keys, and fields that do not have unique names.The potential for insertion anomaly exists because there are entities without primary keys. For example, currently there are 6 types of surgeries listed and if I were to insert a seventh surgery type it would be insertion anomaly without a scheduled patient, physician, and surgery date. Therefore, we would need a separate table with a surgery_code primary key to feature all of the different surgeries our hospital offers. Another example of insertion anomaly that exists within this dataset is the listed physicians. In the unnormalized table, there are four different physicians listed, but let's say we have a new physician in our clinic. If I wanted to add a fifth physician there would be an insertion anomaly if I do not have a corresponding patient, appointment, and surgery. 

**Deletion anomaly:** in the original dataset, all of the information is contained within two columns which can result in potential problems under deletion anomaly. If I were to delete the third row with the patient Joe Korn, without a physician primary key the physician "Olga Kay" would no longer be referenced in the database. Another example of deletion anomaly is if I were to delete the eighth entry/row of "Ashish Patel" the corresponding surgery "Hepatic Resection" would be deleted from the database. If the values had primary keys, they could be referenced in a separate table and this would minimize the potential for deletion anomaly.

**Modification/Updation Anomaly:** Within the unnormalized dataset, if physicians like "Helen Pearson" or "Ashish Patel" resigned because they received a job at another hospital, I would have to update their name three times each in the dataset. This is an example of a modification anomaly in it's unnormalized form. However, of I were to create a physician key for all employees/physicians, I would only need to update the name once every time there's a resignation/termination or new employee.

### B. Normalize this data to 3NF and list any assumptions made during the normalization process. 

As the instructions suggested, I created the normalization tabs in an Excel workbook and created a separate tab for each normal form. 

**Unnormalized**

This is a screenshot of the unnormalized data. As you can see here, the original dataset was featured in two columns with non-atomic values clustered together. 
![](Unnormalized1.png)

**First Normal Form (1NF)**

Within the first normal form, each row is unique and no column contains multiple values. Therefore, my first step was to separate each of the atomic values into their respective columns. I then created three primary keys. The first was "patient_Number (PK)" to uniquely identify each of the patients on our roster. The second was the "physician_code (PK)" to uniquely identify each of our physicians and minimize potential anomalies when making changes to those entries. The third PK was "surgery_code" to have a separate table which identifies the extensive list of surgeries offered by our hospital.

Screenshot of the First Normal Form dataset:
![](firstnormalform.png)

**Second Normal Form (2NF)**

Within the second normal form, I separated the tables to illustrate the new relation between primary keys and foreign keys. The top two tables feature the physician and surgeries primary keys with their respective attributes. The table at the bottom, is the original that will feature one primary key and reference both the physicians and the surgeries through foreign keys.

![](secondnormalform.png)

**Third Normal Form (3NF)**

The third normal form no longer contains functional dependencies. I created a separate table for cities, which provides the automated state and country that corresponds to that city. Since the dataset only featured one city, I only listed Chicago. If the dataset had contained multiple cities, I would've listed all cities with their respective states and countries.

![](thirdnormalform.png)